--- Reddit scripts ---
Since it is a python script, there is no need to compile.
However, the API needs to be downloaded and configured by following the instructions in 
https://praw.readthedocs.org/en/stable/index.html#main-page
To crawl a subreddit, change the contents in r.get_subreddit('')
Run after that run the script through python IDLE

--- Twitter scripts ---
This is written in python too. To run it, you have to first create your own API key
Then you should be able to run the script after that

--- To compile Word2Vec ---
- gcc ./5.4_5.5/5.4_5.5/word2vec.c -o word2vec
- gcc ./5.2/NLP 5.2/distance.c -o distance
- gcc ./5.2/NLP 5.2/regularities.c -o regularities
- gcc ./5.2/NLP 5.2/word-analogy.c -o word-analogy
--- Run codes for section 5.2 ---
- ./word-analogy GoogleNews-vectors-negative300.bin
- ./word-analogy corpus.bin  
- ./distance corpus.bin
- ./regularities corpus.bin
--- Build models for section 5.4 and 5.5 ---
- Build a binary corpus file with word2vec
- Download Weka at http://www.cs.waikato.ac.nz/ml/weka
- Download TextBlob at https://github.com/sloria/textblob-aptagger/tree/master
- Use TextBlob to POS tags training and testing sets
- Open Weka, Choose Explorer, choose an arff file as training set
- Choose FilteredClassifier as classification model
- Under FilterClassifier options, choose LibSVM as the library and StringToWordVector as filter
- Choose an arff file as test set
- Press Start and note down the performance	
- Data files (*.arff, *.csv) at ./5.4_5.5

---  5.6 ---
Step 1: Labeled Data Collection from Twitter
Step 2: All the data stored in subfolder --- "txt"

For normal Data Preprocessing
Step 3: Ran normals.py
Step 4: The result stored in subfolder --- "normals"
Step 5: Converted "normals" subfolder into weka dataset by TextDirectoryLoader
Step 6: Acquired the "text_normals.arff" file
Step 7: Open "text_normals.arff" by Weka
Step 8: Click "Explorer" button
Step 9: In "Preprocess" tag -> Open file... -> Select "text_normals.arff"
Step 10: In "Classify" tag -> Choose Classifier "weka/classifiers/meta/Vote" 
-> Click "Start" button -> Generate Classifier output

For emoticons Data Preprocessing
Step 3: Ran emoticons.py
Step 4: The result stored in subfolder --- "emoticons"
Step 5: Converted "emoticons" subfolder into weka dataset by TextDirectoryLoader
Step 6: Acquired the "text_emoticons.arff" file
Step 7: Open "text_emoticons.arff" by Weka
Step 8: Click "Explorer" button
Step 9: In "Preprocess" tag -> Open file... -> Select "text_emoticons.arff"
Step 10: In "Classify" tag -> Choose Classifier "weka/classifiers/meta/Vote" 
-> Click "Start" button -> Generate Classifier output

Please refer to two PNG files --- "ListOfEmoticons" and "ListOfNormals"
